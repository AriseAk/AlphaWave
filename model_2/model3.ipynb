{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03138eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# This will list all physical devices TensorFlow can see\n",
    "devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if len(devices) > 0:\n",
    "    print(f\"✅ GPU DETECTED: {len(devices)} device(s) found.\")\n",
    "    print(devices)\n",
    "else:\n",
    "    print(\"❌ GPU NOT DETECTED. TensorFlow will use the CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970b2695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy, scipy, sklearn\n",
    "print(\"NumPy:\", numpy.__version__)\n",
    "print(\"SciPy:\", scipy.__version__)\n",
    "print(\"Scikit-learn:\", sklearn.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca39d57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Load Stock Data\n",
    "# ---------------------------\n",
    "stock_symbol = 'AAPL'  # You can change to TSLA, MSFT, etc.\n",
    "data = yf.download(stock_symbol, start='2018-01-01', end='2025-01-01')\n",
    "\n",
    "# Use only the 'Close' price for simplicity\n",
    "close_prices = data['Close'].values.reshape(-1, 1)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Preprocess Data\n",
    "# ---------------------------\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(close_prices)\n",
    "\n",
    "# 80% training, 20% testing split\n",
    "training_data_len = int(len(scaled_data) * 0.8)\n",
    "train_data = scaled_data[:training_data_len]\n",
    "test_data = scaled_data[training_data_len - 60:]  # 60 for lookback\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Create Sequences for LSTM\n",
    "# ---------------------------\n",
    "def create_dataset(dataset, time_step=60):\n",
    "    X, y = [], []\n",
    "    for i in range(time_step, len(dataset)):\n",
    "        X.append(dataset[i - time_step:i, 0])\n",
    "        y.append(dataset[i, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train, y_train = create_dataset(train_data)\n",
    "X_test, y_test = create_dataset(test_data)\n",
    "\n",
    "# Reshape to [samples, time steps, features]\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Build the LSTM Model\n",
    "# ---------------------------\n",
    "model = Sequential([\n",
    "    LSTM(100, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n",
    "    Dropout(0.2),\n",
    "    LSTM(100, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.summary()\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Train the Model\n",
    "# ---------------------------\n",
    "history = model.fit(X_train, y_train, batch_size=64, epochs=25, validation_split=0.1, verbose=1)\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Make Predictions\n",
    "# ---------------------------\n",
    "predictions = model.predict(X_test)\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "y_test_actual = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# ---------------------------\n",
    "# 7. Evaluate Model\n",
    "# ---------------------------\n",
    "rmse = math.sqrt(mean_squared_error(y_test_actual, predictions))\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse:.2f}')\n",
    "\n",
    "# ---------------------------\n",
    "# 8. Visualize Results\n",
    "# ---------------------------\n",
    "train = data[:training_data_len]\n",
    "valid = data[training_data_len:]\n",
    "valid['Predictions'] = predictions\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(f'{stock_symbol} Stock Price Prediction (AlphaWave LSTM)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price (USD)')\n",
    "plt.plot(train['Close'], label='Training Data')\n",
    "plt.plot(valid['Close'], label='Actual Price', color='blue')\n",
    "plt.plot(valid['Predictions'], label='Predicted Price', color='red')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# 9. Predict Next 30 Days (Optional)\n",
    "# ---------------------------\n",
    "last_60_days = scaled_data[-60:]\n",
    "next_input = np.array(last_60_days).reshape(1, 60, 1)\n",
    "next_30_days = []\n",
    "\n",
    "for _ in range(30):\n",
    "    next_price = model.predict(next_input)[0, 0]\n",
    "    next_30_days.append(next_price)\n",
    "    next_input = np.append(next_input[:, 1:, :], [[[next_price]]], axis=1)\n",
    "\n",
    "# Inverse scale predictions\n",
    "next_30_days = scaler.inverse_transform(np.array(next_30_days).reshape(-1, 1))\n",
    "\n",
    "# Plot future prediction\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, 31), next_30_days, marker='o', color='green')\n",
    "plt.title(f'{stock_symbol} - Predicted Prices for Next 30 Days (AlphaWave)')\n",
    "plt.xlabel('Days Ahead')\n",
    "plt.ylabel('Predicted Price (USD)')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dbcf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AlphaWave v2 – Universal Multi-Stock LSTM Predictor (with model saving)\n",
    "# Author: AriseAK\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib  # for saving scaler\n",
    "import math\n",
    "import os\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Configuration\n",
    "# ---------------------------\n",
    "stock_symbols = [\n",
    "    'AAPL', 'MSFT', 'TSLA', 'GOOG', 'AMZN', 'NVDA', 'META', 'NFLX', 'AMD', 'INTC',\n",
    "    'ADBE', 'ORCL', 'IBM', 'PYPL', 'CRM', 'CSCO', 'PEP', 'KO', 'NKE', 'DIS',\n",
    "    'JPM', 'BAC', 'V', 'MA', 'WMT'\n",
    "]  # 25 major companies\n",
    "\n",
    "start_date = '2018-01-01'\n",
    "end_date = '2025-01-01'\n",
    "model_dir = \"alphawave_model\"\n",
    "\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Download and Combine Data\n",
    "# ---------------------------\n",
    "print(\"Fetching data for all stocks...\")\n",
    "data = yf.download(stock_symbols, start=start_date, end=end_date)['Close']\n",
    "data = data.dropna(axis=1)  # remove columns with missing values\n",
    "\n",
    "print(f\"Final number of stocks used: {len(data.columns)}\")\n",
    "print(\"Stocks:\", list(data.columns))\n",
    "\n",
    "# Scale the entire dataframe\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data.columns, index=data.index)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Prepare Training and Testing Data\n",
    "# ---------------------------\n",
    "training_data_len = int(len(scaled_df) * 0.8)\n",
    "train_data = scaled_df.iloc[:training_data_len]\n",
    "test_data = scaled_df.iloc[training_data_len - 60:]  # overlap for sequence\n",
    "\n",
    "def create_multifeature_dataset(dataset, time_step=60):\n",
    "    X, y = [], []\n",
    "    for i in range(time_step, len(dataset)):\n",
    "        X.append(dataset.iloc[i - time_step:i].values)\n",
    "        y.append(dataset.iloc[i].values)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train, y_train = create_multifeature_dataset(train_data)\n",
    "X_test, y_test = create_multifeature_dataset(test_data)\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Build the Universal LSTM Model\n",
    "# ---------------------------\n",
    "model = Sequential([\n",
    "    LSTM(128, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dropout(0.3),\n",
    "    LSTM(128, return_sequences=False),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(y_train.shape[1])  # predict all stocks\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.summary()\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Train the Model\n",
    "# ---------------------------\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=64, validation_split=0.1, verbose=1)\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Save Model and Scaler\n",
    "# ---------------------------\n",
    "model_path_h5 = os.path.join(model_dir, \"alphawave_universal_model.h5\")\n",
    "model_path_keras = os.path.join(model_dir, \"alphawave_universal_model.keras\")\n",
    "scaler_path = os.path.join(model_dir, \"alphawave_scaler.pkl\")\n",
    "\n",
    "model.save(model_path_h5)\n",
    "model.save(model_path_keras)\n",
    "joblib.dump(scaler, scaler_path)\n",
    "\n",
    "print(f\"\\n✅ Model saved successfully to:\\n - {model_path_h5}\\n - {model_path_keras}\")\n",
    "print(f\"✅ Scaler saved to: {scaler_path}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 7. Evaluate and Plot Sample Stocks\n",
    "# ---------------------------\n",
    "predictions = model.predict(X_test)\n",
    "predictions_rescaled = scaler.inverse_transform(predictions)\n",
    "y_test_actual = scaler.inverse_transform(y_test)\n",
    "\n",
    "rmse_per_stock = {}\n",
    "for idx, stock in enumerate(data.columns):\n",
    "    rmse = math.sqrt(mean_squared_error(y_test_actual[:, idx], predictions_rescaled[:, idx]))\n",
    "    rmse_per_stock[stock] = rmse\n",
    "\n",
    "print(\"\\nRMSE per Stock:\")\n",
    "for stock, score in rmse_per_stock.items():\n",
    "    print(f\"{stock}: {score:.2f}\")\n",
    "\n",
    "sample_stocks = ['AAPL', 'TSLA', 'GOOG', 'AMZN']\n",
    "for stock in sample_stocks:\n",
    "    idx = list(data.columns).index(stock)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(data.index[training_data_len:], y_test_actual[:, idx], label='Actual', color='blue')\n",
    "    plt.plot(data.index[training_data_len:], predictions_rescaled[:, idx], label='Predicted', color='red')\n",
    "    plt.title(f'{stock} Price Prediction (AlphaWave Universal LSTM)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Price (USD)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# 8. Predict Next 30 Days for All Stocks\n",
    "# ---------------------------\n",
    "last_60_days = scaled_df.iloc[-60:].values.reshape(1, 60, len(data.columns))\n",
    "future_predictions = []\n",
    "\n",
    "for _ in range(30):\n",
    "    next_pred = model.predict(last_60_days)[0]\n",
    "    future_predictions.append(next_pred)\n",
    "    next_input = np.append(last_60_days[:, 1:, :], [[next_pred]], axis=1)\n",
    "    last_60_days = next_input\n",
    "\n",
    "future_predictions = scaler.inverse_transform(np.array(future_predictions))\n",
    "\n",
    "for stock in sample_stocks:\n",
    "    idx = list(data.columns).index(stock)\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(range(1, 31), future_predictions[:, idx], marker='o', color='green')\n",
    "    plt.title(f'{stock} - 30-Day Forecast (AlphaWave Universal LSTM)')\n",
    "    plt.xlabel('Days Ahead')\n",
    "    plt.ylabel('Predicted Price (USD)')\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e0778c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AlphaWave v3 – Optimized Universal LSTM (No model saving)\n",
    "# Author: AriseAK\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Configuration\n",
    "# ---------------------------\n",
    "stock_symbols = [\n",
    "    'AAPL', 'MSFT', 'TSLA', 'GOOG', 'AMZN', 'NVDA', 'META', 'NFLX', 'AMD', 'INTC',\n",
    "    'ADBE', 'ORCL', 'IBM', 'PYPL', 'CRM', 'CSCO', 'PEP', 'KO', 'NKE', 'DIS',\n",
    "    'JPM', 'BAC', 'V', 'MA', 'WMT'\n",
    "]\n",
    "start_date = '2018-01-01'\n",
    "end_date = '2025-01-01'\n",
    "lookback = 90\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Helper Functions\n",
    "# ---------------------------\n",
    "\n",
    "def compute_rsi(series, period=14):\n",
    "    delta = series.diff(1)\n",
    "    gain = delta.where(delta > 0, 0.0)\n",
    "    loss = -delta.where(delta < 0, 0.0)\n",
    "    avg_gain = gain.rolling(window=period, min_periods=1).mean()\n",
    "    avg_loss = loss.rolling(window=period, min_periods=1).mean()\n",
    "    rs = avg_gain / (avg_loss + 1e-10)\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def create_multifeature_dataset(df, time_step=60):\n",
    "    X, y = [], []\n",
    "    for i in range(time_step, len(df)):\n",
    "        X.append(df.iloc[i - time_step:i].values)\n",
    "        y.append(df.iloc[i].values)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Data Loading & Feature Engineering\n",
    "# ---------------------------\n",
    "print(\"Fetching stock data...\")\n",
    "data = yf.download(stock_symbols, start=start_date, end=end_date)['Close']\n",
    "data = data.dropna(axis=1)\n",
    "\n",
    "# Add technical indicators for each stock\n",
    "print(\"Adding technical indicators...\")\n",
    "features = pd.DataFrame(index=data.index)\n",
    "for col in data.columns:\n",
    "    df = pd.DataFrame()\n",
    "    df[f'{col}_Close'] = data[col]\n",
    "    df[f'{col}_MA7'] = data[col].rolling(window=7).mean()\n",
    "    df[f'{col}_MA21'] = data[col].rolling(window=21).mean()\n",
    "    df[f'{col}_RSI'] = compute_rsi(data[col])\n",
    "    df[f'{col}_Volatility'] = data[col].rolling(window=7).std()\n",
    "    df = df.fillna(method='bfill').fillna(method='ffill')\n",
    "\n",
    "    # Per-stock MinMax scaling (0–1)\n",
    "    scaler = MinMaxScaler()\n",
    "    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns, index=df.index)\n",
    "    features = pd.concat([features, df_scaled], axis=1)\n",
    "\n",
    "print(f\"Final feature count: {features.shape[1]} (per stock indicators included)\")\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Train-Test Split & Dataset Creation\n",
    "# ---------------------------\n",
    "training_len = int(len(features) * 0.8)\n",
    "train_data = features.iloc[:training_len]\n",
    "test_data = features.iloc[training_len - lookback:]  # overlap\n",
    "\n",
    "X_train, y_train = create_multifeature_dataset(train_data, time_step=lookback)\n",
    "X_test, y_test = create_multifeature_dataset(test_data, time_step=lookback)\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Build Optimized LSTM Model\n",
    "# ---------------------------\n",
    "model = Sequential([\n",
    "    LSTM(128, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    LSTM(128, return_sequences=True),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    LSTM(64, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(y_train.shape[1])\n",
    "])\n",
    "\n",
    "optimizer = Adam(learning_rate=0.0003)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "model.summary()\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Train with EarlyStopping\n",
    "# ---------------------------\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=60,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    shuffle=False,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 7. Evaluate Performance\n",
    "# ---------------------------\n",
    "predictions = model.predict(X_test)\n",
    "y_test_actual = y_test  # Already scaled per-stock\n",
    "rmse = math.sqrt(mean_squared_error(y_test.flatten(), predictions.flatten()))\n",
    "print(f\"\\nOverall RMSE (all stocks/features): {rmse:.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 8. Visualize Key Stocks\n",
    "# ---------------------------\n",
    "# Extract key columns to visualize (close price columns only)\n",
    "close_cols = [col for col in features.columns if '_Close' in col]\n",
    "num_features_per_stock = len(features.columns) // len(close_cols)\n",
    "\n",
    "for stock in ['AAPL', 'TSLA', 'GOOG', 'AMZN']:\n",
    "    col = f'{stock}_Close'\n",
    "    idx = list(features.columns).index(col)\n",
    "\n",
    "    actual = y_test[:, idx]\n",
    "    predicted = predictions[:, idx]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(features.index[training_len:], actual, label='Actual', color='blue')\n",
    "    plt.plot(features.index[training_len:], predicted, label='Predicted', color='red')\n",
    "    plt.title(f'{stock} Price Prediction (AlphaWave v3 Optimized Universal LSTM)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Normalized Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# 9. Predict Next 30 Days for All Stocks\n",
    "# ---------------------------\n",
    "last_seq = features.iloc[-lookback:].values.reshape(1, lookback, features.shape[1])\n",
    "future_preds = []\n",
    "\n",
    "for _ in range(30):\n",
    "    pred = model.predict(last_seq)[0]\n",
    "    future_preds.append(pred)\n",
    "    last_seq = np.append(last_seq[:, 1:, :], [[pred]], axis=1)\n",
    "\n",
    "future_preds = np.array(future_preds)\n",
    "\n",
    "# Plot 30-day forecasts (normalized values)\n",
    "for stock in ['AAPL', 'TSLA', 'GOOG', 'AMZN']:\n",
    "    col = f'{stock}_Close'\n",
    "    idx = list(features.columns).index(col)\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(range(1, 31), future_preds[:, idx], color='green', marker='o')\n",
    "    plt.title(f'{stock} - 30-Day Forecast (AlphaWave v3 Optimized LSTM)')\n",
    "    plt.xlabel('Days Ahead')\n",
    "    plt.ylabel('Normalized Predicted Price')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n✅ Training complete. Model not saved (as requested).\")\n",
    "print(\"You can later save it anytime using:\")\n",
    "print(\"    model.save('alphawave_model_v3.h5')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074b3970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AlphaWave v4 – Hybrid Universal LSTM (Stock-Aware, Volume, Market Index)\n",
    "# Author: AriseAK\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Dropout, BatchNormalization, Concatenate, Embedding, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Configuration\n",
    "# ---------------------------\n",
    "stock_symbols = [\n",
    "    'AAPL', 'MSFT', 'TSLA', 'GOOG', 'AMZN', 'NVDA', 'META', 'NFLX', 'AMD', 'INTC',\n",
    "    'ADBE', 'ORCL', 'IBM', 'PYPL', 'CRM', 'CSCO', 'PEP', 'KO', 'NKE', 'DIS',\n",
    "    'JPM', 'BAC', 'V', 'MA', 'WMT'\n",
    "]\n",
    "start_date = '2015-01-01'\n",
    "end_date = '2025-01-01'\n",
    "lookback = 120\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Load Stock & Market Data\n",
    "# ---------------------------\n",
    "print(\"Fetching data for all stocks...\")\n",
    "data = yf.download(stock_symbols, start=start_date, end=end_date)\n",
    "close_df = data['Close']\n",
    "volume_df = data['Volume']\n",
    "\n",
    "# Drop missing columns if any stock has incomplete data\n",
    "close_df = close_df.dropna(axis=1)\n",
    "volume_df = volume_df[close_df.columns]\n",
    "\n",
    "# Add SPY (S&P500) as a global index feature\n",
    "print(\"Adding market index (SPY)...\")\n",
    "spy = yf.download('SPY', start=start_date, end=end_date)['Close']\n",
    "spy = spy.reindex(close_df.index).fillna(method='ffill')\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Feature Engineering\n",
    "# ---------------------------\n",
    "# Combine close, volume, and SPY\n",
    "features = pd.DataFrame(index=close_df.index)\n",
    "for stock in close_df.columns:\n",
    "    features[f'{stock}_Close'] = close_df[stock]\n",
    "    features[f'{stock}_Volume'] = volume_df[stock]\n",
    "\n",
    "features['SPY_Close'] = spy\n",
    "\n",
    "# Fill missing\n",
    "features = features.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# Standard scaling (better than MinMax for financial data)\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(features)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=features.columns, index=features.index)\n",
    "\n",
    "print(f\"Final feature matrix shape: {scaled_df.shape}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Create Sequences\n",
    "# ---------------------------\n",
    "def create_dataset(dataset, stocks, time_step=60):\n",
    "    X, y, stock_ids = [], [], []\n",
    "    for stock_idx, stock in enumerate(stocks):\n",
    "        close_col = f'{stock}_Close'\n",
    "        vol_col = f'{stock}_Volume'\n",
    "\n",
    "        stock_features = dataset[[close_col, vol_col, 'SPY_Close']].values  # 3 features per time step\n",
    "        for i in range(time_step, len(stock_features)):\n",
    "            X.append(stock_features[i - time_step:i])\n",
    "            y.append(stock_features[i, 0])  # predict closing price\n",
    "            stock_ids.append(stock_idx)  # one-hot or embedding index\n",
    "    return np.array(X), np.array(y), np.array(stock_ids)\n",
    "\n",
    "print(\"Creating training sequences...\")\n",
    "training_len = int(len(scaled_df) * 0.8)\n",
    "train_data = scaled_df.iloc[:training_len]\n",
    "test_data = scaled_df.iloc[training_len - lookback:]\n",
    "\n",
    "X_train, y_train, stock_train_ids = create_dataset(train_data, close_df.columns, lookback)\n",
    "X_test, y_test, stock_test_ids = create_dataset(test_data, close_df.columns, lookback)\n",
    "\n",
    "print(\"X_train:\", X_train.shape, \"X_test:\", X_test.shape)\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Build Stock-Aware Hybrid LSTM\n",
    "# ---------------------------\n",
    "# Inputs\n",
    "price_input = Input(shape=(X_train.shape[1], X_train.shape[2]), name='price_input')\n",
    "stock_input = Input(shape=(1,), name='stock_input')\n",
    "\n",
    "# Stock Embedding (gives the model stock identity awareness)\n",
    "embedding = Embedding(input_dim=len(close_df.columns), output_dim=5, input_length=1)(stock_input)\n",
    "embedding = Flatten()(embedding)\n",
    "\n",
    "# LSTM feature extractor\n",
    "x = LSTM(128, return_sequences=True)(price_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = LSTM(64, return_sequences=False)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# Combine LSTM output + stock identity\n",
    "combined = Concatenate()([x, embedding])\n",
    "combined = Dense(64, activation='relu')(combined)\n",
    "combined = Dropout(0.2)(combined)\n",
    "output = Dense(1)(combined)\n",
    "\n",
    "model = Model(inputs=[price_input, stock_input], outputs=output)\n",
    "model.compile(optimizer=Adam(learning_rate=0.0003), loss='mean_squared_error')\n",
    "model.summary()\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Train Model\n",
    "# ---------------------------\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train, stock_train_ids],\n",
    "    y_train,\n",
    "    epochs=80,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    shuffle=False,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 7. Evaluate & Predict\n",
    "# ---------------------------\n",
    "predictions = model.predict([X_test, stock_test_ids])\n",
    "rmse = math.sqrt(mean_squared_error(y_test, predictions))\n",
    "print(f\"\\nOverall RMSE: {rmse:.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 8. Plot Predictions for Sample Stocks\n",
    "# ---------------------------\n",
    "from sklearn.metrics import r2_score\n",
    "unique_stocks = close_df.columns[:4]  # visualize a few\n",
    "\n",
    "for stock in unique_stocks:\n",
    "    idx = np.where(stock_test_ids == list(close_df.columns).index(stock))[0]\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(y_test[idx], label='Actual', color='blue')\n",
    "    plt.plot(predictions[idx], label='Predicted', color='red')\n",
    "    plt.title(f\"{stock} Price Prediction (AlphaWave v4 Hybrid Universal LSTM)\")\n",
    "    plt.xlabel(\"Days Ahead\")\n",
    "    plt.ylabel(\"Standardized Price\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# 9. Optional: 30-Day Forecast\n",
    "# ---------------------------\n",
    "# For a single stock, e.g., AAPL\n",
    "stock_idx = list(close_df.columns).index('AAPL')\n",
    "last_seq = scaled_df[[f'AAPL_Close', f'AAPL_Volume', 'SPY_Close']].iloc[-lookback:].values.reshape(1, lookback, 3)\n",
    "stock_input = np.array([[stock_idx]])\n",
    "future_preds = []\n",
    "\n",
    "\n",
    "last_seq = np.array(last_seq, dtype=np.float32)\n",
    "stock_input = np.array([[stock_idx]], dtype=np.int32)\n",
    "\n",
    "\n",
    "for _ in range(30):\n",
    "    pred = model.predict([last_seq, stock_input], verbose=0)[0, 0]\n",
    "    future_preds.append(pred)\n",
    "    new_frame = np.array([[[pred, last_seq[0, -1, 1], last_seq[0, -1, 2]]]], dtype=np.float32)\n",
    "    last_seq = np.append(last_seq[:, 1:, :], new_frame, axis=1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(range(1,31), future_preds, color='green', marker='o')\n",
    "plt.title(\"AAPL - 30-Day Forecast (AlphaWave v4 Hybrid LSTM)\")\n",
    "plt.xlabel(\"Days Ahead\")\n",
    "plt.ylabel(\"Standardized Predicted Price\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Training complete — AlphaWave v4 is now stock-aware and market-contextualized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba1017f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AlphaWave v4.1 – Hybrid LSTM (10 Stocks, Per-Stock Scaling)\n",
    "# Author: AriseAK\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Dropout, BatchNormalization, Concatenate, Embedding, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Config\n",
    "# ---------------------------\n",
    "stock_symbols = ['AAPL', 'MSFT', 'AMZN', 'GOOG', 'NVDA', 'META', 'TSLA', 'AMD', 'ADBE', 'NFLX']\n",
    "start_date = '2015-01-01'\n",
    "end_date = '2025-01-01'\n",
    "lookback = 90  # shorter window = more responsive\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Load Stock & Market Data\n",
    "# ---------------------------\n",
    "print(\"Fetching stock data...\")\n",
    "data = yf.download(stock_symbols, start=start_date, end=end_date)\n",
    "close_df = data['Close']\n",
    "volume_df = data['Volume']\n",
    "\n",
    "# Drop incomplete stocks\n",
    "close_df = close_df.dropna(axis=1)\n",
    "volume_df = volume_df[close_df.columns]\n",
    "\n",
    "# Add SPY as market context\n",
    "print(\"Adding SPY (S&P500 Index)...\")\n",
    "spy = yf.download('SPY', start=start_date, end=end_date)['Close']\n",
    "spy = spy.reindex(close_df.index).fillna(method='ffill')\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Per-Stock Standard Scaling\n",
    "# ---------------------------\n",
    "scalers = {}\n",
    "scaled_close = pd.DataFrame(index=close_df.index)\n",
    "scaled_volume = pd.DataFrame(index=close_df.index)\n",
    "\n",
    "for stock in close_df.columns:\n",
    "    scaler = StandardScaler()\n",
    "    scaled_close[stock] = scaler.fit_transform(close_df[stock].values.reshape(-1, 1)).flatten()\n",
    "    scaled_volume[stock] = StandardScaler().fit_transform(volume_df[stock].values.reshape(-1, 1)).flatten()\n",
    "    scalers[stock] = scaler\n",
    "\n",
    "spy_scaler = StandardScaler()\n",
    "scaled_spy = spy_scaler.fit_transform(spy.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Combine All Features\n",
    "# ---------------------------\n",
    "scaled_features = pd.DataFrame(index=close_df.index)\n",
    "for stock in close_df.columns:\n",
    "    scaled_features[f'{stock}_Close'] = scaled_close[stock]\n",
    "    scaled_features[f'{stock}_Volume'] = scaled_volume[stock]\n",
    "scaled_features['SPY_Close'] = scaled_spy\n",
    "\n",
    "print(\"Final feature matrix shape:\", scaled_features.shape)\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Create Sequences\n",
    "# ---------------------------\n",
    "def create_dataset(dataset, stocks, time_step=60):\n",
    "    X, y, stock_ids = [], [], []\n",
    "    for stock_idx, stock in enumerate(stocks):\n",
    "        close_col = f'{stock}_Close'\n",
    "        vol_col = f'{stock}_Volume'\n",
    "        stock_features = dataset[[close_col, vol_col, 'SPY_Close']].values\n",
    "        \n",
    "        for i in range(time_step, len(stock_features)):\n",
    "            X.append(stock_features[i - time_step:i])\n",
    "            y.append(stock_features[i, 0])  # predict Close\n",
    "            stock_ids.append(stock_idx)\n",
    "    return np.array(X), np.array(y), np.array(stock_ids)\n",
    "\n",
    "train_len = int(len(scaled_features) * 0.8)\n",
    "train_data = scaled_features.iloc[:train_len]\n",
    "test_data = scaled_features.iloc[train_len - lookback:]\n",
    "\n",
    "X_train, y_train, stock_train_ids = create_dataset(train_data, close_df.columns, lookback)\n",
    "X_test, y_test, stock_test_ids = create_dataset(test_data, close_df.columns, lookback)\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Build Hybrid Model\n",
    "# ---------------------------\n",
    "price_input = Input(shape=(X_train.shape[1], X_train.shape[2]), name='price_input')\n",
    "stock_input = Input(shape=(1,), name='stock_input')\n",
    "\n",
    "# Stock embedding\n",
    "embedding = Embedding(input_dim=len(close_df.columns), output_dim=5, input_length=1)(stock_input)\n",
    "embedding = Flatten()(embedding)\n",
    "\n",
    "# LSTM feature extractor\n",
    "x = LSTM(128, return_sequences=True)(price_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "x = LSTM(64, return_sequences=False)(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "# Merge LSTM + embedding\n",
    "combined = Concatenate()([x, embedding])\n",
    "combined = Dense(64, activation='relu')(combined)\n",
    "combined = Dropout(0.1)(combined)\n",
    "output = Dense(1)(combined)\n",
    "\n",
    "model = Model(inputs=[price_input, stock_input], outputs=output)\n",
    "model.compile(optimizer=Adam(learning_rate=0.0003), loss='mean_squared_error')\n",
    "model.summary()\n",
    "\n",
    "# ---------------------------\n",
    "# 7. Train Model\n",
    "# ---------------------------\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train, stock_train_ids],\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    shuffle=False,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 8. Evaluate\n",
    "# ---------------------------\n",
    "predictions = model.predict([X_test, stock_test_ids])\n",
    "rmse = math.sqrt(mean_squared_error(y_test, predictions))\n",
    "print(f\"Overall RMSE: {rmse:.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 9. Visualize Sample Stocks\n",
    "# ---------------------------\n",
    "unique_stocks = close_df.columns[:4]  # visualize 4\n",
    "\n",
    "for stock in unique_stocks:\n",
    "    idx = np.where(stock_test_ids == list(close_df.columns).index(stock))[0]\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(y_test[idx], label='Actual', color='blue')\n",
    "    plt.plot(predictions[idx], label='Predicted', color='red')\n",
    "    plt.title(f\"{stock} Price Prediction (AlphaWave v4.1 Hybrid LSTM)\")\n",
    "    plt.xlabel(\"Days Ahead\")\n",
    "    plt.ylabel(\"Standardized Price\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# 10. 30-Day Forecast Example\n",
    "# ---------------------------\n",
    "print(\"\\nGenerating 30-day forecast for AAPL...\")\n",
    "\n",
    "stock_idx = list(close_df.columns).index('AAPL')\n",
    "last_seq = scaled_features[[f'AAPL_Close', f'AAPL_Volume', 'SPY_Close']].iloc[-lookback:].values\n",
    "last_seq = np.array(last_seq, dtype=np.float32).reshape(1, lookback, 3)\n",
    "stock_input = np.array([[stock_idx]], dtype=np.int32)\n",
    "\n",
    "future_preds = []\n",
    "\n",
    "for _ in range(30):\n",
    "    pred = model.predict([last_seq, stock_input], verbose=0)[0, 0]\n",
    "    future_preds.append(pred)\n",
    "    new_frame = np.array([[[pred, last_seq[0, -1, 1], last_seq[0, -1, 2]]]], dtype=np.float32)\n",
    "    last_seq = np.append(last_seq[:, 1:, :], new_frame, axis=1)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(range(1,31), future_preds, color='green', marker='o')\n",
    "plt.title(\"AAPL - 30-Day Forecast (AlphaWave v4.1)\")\n",
    "plt.xlabel(\"Days Ahead\")\n",
    "plt.ylabel(\"Standardized Predicted Price\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66148922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AlphaWave vX – Unified Model with Stock Embeddings\n",
    "# Author: AriseAK\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Dropout, BatchNormalization, Embedding, Flatten, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Config\n",
    "# ---------------------------\n",
    "stock_symbols = [\n",
    "    'AAPL', 'MSFT', 'AMZN', 'GOOG', 'NVDA', 'META', 'TSLA', \n",
    "    'AMD', 'ADBE', 'NFLX', 'INTC', 'IBM', 'PYPL', 'ORCL', \n",
    "    'CSCO', 'CRM', 'QCOM', 'AVGO', 'TXN', 'SHOP', 'UBER', 'SNOW', \n",
    "    'BABA', 'PLTR', 'COIN'\n",
    "]\n",
    "start_date = '2015-01-01'\n",
    "end_date = '2025-01-01'\n",
    "lookback = 90  # sequence window\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Load Stock & Market Data\n",
    "# ---------------------------\n",
    "print(\"Fetching stock data...\")\n",
    "data = yf.download(stock_symbols, start=start_date, end=end_date)\n",
    "close_df = data['Close']\n",
    "volume_df = data['Volume']\n",
    "close_df = close_df.dropna(axis=1)\n",
    "volume_df = volume_df[close_df.columns]\n",
    "\n",
    "print(\"Adding SPY (S&P500 Index)...\")\n",
    "spy = yf.download('SPY', start=start_date, end=end_date)['Close']\n",
    "spy = spy.reindex(close_df.index).fillna(method='ffill')\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Per-Stock Scaling\n",
    "# ---------------------------\n",
    "scalers = {}\n",
    "scaled_close, scaled_volume = pd.DataFrame(index=close_df.index), pd.DataFrame(index=close_df.index)\n",
    "\n",
    "for stock in close_df.columns:\n",
    "    scaler = StandardScaler()\n",
    "    scaled_close[stock] = scaler.fit_transform(close_df[stock].values.reshape(-1, 1)).flatten()\n",
    "    scaled_volume[stock] = StandardScaler().fit_transform(volume_df[stock].values.reshape(-1, 1)).flatten()\n",
    "    scalers[stock] = scaler\n",
    "\n",
    "spy_scaler = StandardScaler()\n",
    "scaled_spy = spy_scaler.fit_transform(spy.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Create Dataset\n",
    "# ---------------------------\n",
    "def create_dataset(dataset, stocks, time_step=60):\n",
    "    X, y, stock_ids = [], [], []\n",
    "    for stock_idx, stock in enumerate(stocks):\n",
    "        close_col = f'{stock}_Close'\n",
    "        vol_col = f'{stock}_Volume'\n",
    "        stock_features = dataset[[close_col, vol_col, 'SPY_Close']].values\n",
    "        \n",
    "        for i in range(time_step, len(stock_features)):\n",
    "            X.append(stock_features[i - time_step:i])\n",
    "            y.append(stock_features[i, 0])\n",
    "            stock_ids.append(stock_idx)\n",
    "    return np.array(X), np.array(y), np.array(stock_ids)\n",
    "\n",
    "scaled_features = pd.DataFrame(index=close_df.index)\n",
    "for stock in close_df.columns:\n",
    "    scaled_features[f'{stock}_Close'] = scaled_close[stock]\n",
    "    scaled_features[f'{stock}_Volume'] = scaled_volume[stock]\n",
    "scaled_features['SPY_Close'] = scaled_spy\n",
    "\n",
    "train_len = int(len(scaled_features) * 0.8)\n",
    "train_data = scaled_features.iloc[:train_len]\n",
    "test_data = scaled_features.iloc[train_len - lookback:]\n",
    "\n",
    "X_train, y_train, stock_train_ids = create_dataset(train_data, close_df.columns, lookback)\n",
    "X_test, y_test, stock_test_ids = create_dataset(test_data, close_df.columns, lookback)\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Build Unified Model\n",
    "# ---------------------------\n",
    "price_input = Input(shape=(X_train.shape[1], X_train.shape[2]), name='price_input')\n",
    "stock_input = Input(shape=(1,), name='stock_input')\n",
    "\n",
    "# Stock embedding layer – each stock gets an 8D learned vector\n",
    "embedding = Embedding(input_dim=len(close_df.columns), output_dim=8, input_length=1, name='stock_embedding')(stock_input)\n",
    "embedding = Flatten()(embedding)\n",
    "\n",
    "# Shared LSTM backbone\n",
    "x = LSTM(128, return_sequences=True)(price_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "x = LSTM(64, return_sequences=False)(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "# Merge temporal features with stock personality\n",
    "combined = Concatenate()([x, embedding])\n",
    "combined = Dense(64, activation='relu')(combined)\n",
    "combined = Dropout(0.1)(combined)\n",
    "output = Dense(1)(combined)\n",
    "\n",
    "model = Model(inputs=[price_input, stock_input], outputs=output)\n",
    "model.compile(optimizer=Adam(learning_rate=0.0003), loss='mean_squared_error')\n",
    "model.summary()\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Train\n",
    "# ---------------------------\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train, stock_train_ids],\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    shuffle=True,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 7. Evaluate\n",
    "# ---------------------------\n",
    "predictions = model.predict([X_test, stock_test_ids])\n",
    "rmse = math.sqrt(mean_squared_error(y_test, predictions))\n",
    "print(f\"Overall RMSE: {rmse:.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 8. Visualize Few Stocks\n",
    "# ---------------------------\n",
    "unique_stocks = close_df.columns[:4]\n",
    "\n",
    "for stock in unique_stocks:\n",
    "    idx = np.where(stock_test_ids == list(close_df.columns).index(stock))[0]\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(y_test[idx], label='Actual', color='blue')\n",
    "    plt.plot(predictions[idx], label='Predicted', color='red')\n",
    "    plt.title(f\"{stock} Stock Prediction (AlphaWave vX Unified Model)\")\n",
    "    plt.xlabel(\"Days\")\n",
    "    plt.ylabel(\"Standardized Price\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# 9. Extract and Save Stock Embeddings\n",
    "# ---------------------------\n",
    "embedding_weights = model.get_layer('stock_embedding').get_weights()[0]\n",
    "stock_embeddings = pd.DataFrame(embedding_weights, index=close_df.columns)\n",
    "print(\"\\nLearned Stock Embeddings (first 5):\")\n",
    "print(stock_embeddings.head())\n",
    "\n",
    "# ---------------------------\n",
    "# 10. Compute Stock Similarities\n",
    "# ---------------------------\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sim_matrix = cosine_similarity(stock_embeddings.values)\n",
    "sim_df = pd.DataFrame(sim_matrix, index=close_df.columns, columns=close_df.columns)\n",
    "\n",
    "print(\"\\nTop 5 Most Similar Stocks:\")\n",
    "for stock in close_df.columns[:5]:\n",
    "    similar = sim_df[stock].sort_values(ascending=False)[1:4]\n",
    "    print(f\"{stock}: {', '.join(similar.index)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7a1c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# AlphaWave vX.3 - Simplified, Regularized, Robust LSTM\n",
    "# Handles overfitting + supports multiple stocks\n",
    "# ==============================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Dropout, GaussianNoise\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1️⃣ Load Stock Data (10 stocks)\n",
    "# ------------------------------------------------------------\n",
    "tickers = [\n",
    "    'AAPL', 'MSFT', 'AMZN', 'GOOG', 'NVDA', 'META', 'TSLA', 'AMD', 'ADBE', 'NFLX',\n",
    "    'INTC', 'IBM', 'PYPL', 'ORCL', 'CSCO', 'CRM', 'QCOM', 'AVGO', 'TXN', 'SHOP',\n",
    "    'UBER', 'SNOW', 'BABA', 'PLTR', 'COIN'\n",
    "]\n",
    "\n",
    "\n",
    "data_dict = {}\n",
    "for ticker in tickers:\n",
    "    print(f\"Downloading data for {ticker}...\")\n",
    "    data_dict[ticker] = yf.download(ticker, start='2015-01-01', end='2025-01-01')\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2️⃣ Preprocess Data (Close + Volume)\n",
    "# ------------------------------------------------------------\n",
    "def preprocess_data(df):\n",
    "    df = df[['Close', 'Volume']].dropna()\n",
    "    scaler = StandardScaler()\n",
    "    scaled = scaler.fit_transform(df)\n",
    "    return scaled, scaler\n",
    "\n",
    "scaled_data = {}\n",
    "scalers = {}\n",
    "\n",
    "for t in tickers:\n",
    "    scaled_data[t], scalers[t] = preprocess_data(data_dict[t])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3️⃣ Create Sequential Dataset (Lookback = 90 days)\n",
    "# ------------------------------------------------------------\n",
    "def create_sequences(data, seq_len=90):\n",
    "    X, y = [], []\n",
    "    for i in range(seq_len, len(data)):\n",
    "        X.append(data[i - seq_len:i])\n",
    "        y.append(data[i, 0])  # Predict 'Close'\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "seq_len = 90\n",
    "X_all, y_all = [], []\n",
    "\n",
    "for t in tickers:\n",
    "    X, y = create_sequences(scaled_data[t], seq_len)\n",
    "    X_all.append(X)\n",
    "    y_all.append(y)\n",
    "\n",
    "X_all = np.concatenate(X_all)\n",
    "y_all = np.concatenate(y_all)\n",
    "\n",
    "print(f\"\\nCombined dataset shape: {X_all.shape}, {y_all.shape}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4️⃣ Time-based Split (Train 70% / Val 15% / Test 15%)\n",
    "# ------------------------------------------------------------\n",
    "train_size = int(len(X_all) * 0.7)\n",
    "val_size = int(len(X_all) * 0.15)\n",
    "\n",
    "X_train = X_all[:train_size]\n",
    "y_train = y_all[:train_size]\n",
    "X_val = X_all[train_size:train_size + val_size]\n",
    "y_val = y_all[train_size:train_size + val_size]\n",
    "X_test = X_all[train_size + val_size:]\n",
    "y_test = y_all[train_size + val_size:]\n",
    "\n",
    "print(f\"Train: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5️⃣ Build Simplified & Regularized LSTM Model\n",
    "# ------------------------------------------------------------\n",
    "input_layer = Input(shape=(seq_len, 2))\n",
    "x = GaussianNoise(0.01)(input_layer)  # Stabilize inputs\n",
    "\n",
    "# ✅ Single LSTM layer (simplified)\n",
    "x = LSTM(64, dropout=0.2, kernel_regularizer=l2(0.001))(x)\n",
    "\n",
    "\n",
    "# ✅ Light dense head\n",
    "x = Dense(32, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "output_layer = Dense(1)(x)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6️⃣ Train with EarlyStopping (Prevent Overfitting)\n",
    "# ------------------------------------------------------------\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7️⃣ Evaluate Model (RMSE)\n",
    "# ------------------------------------------------------------\n",
    "train_pred = model.predict(X_train)\n",
    "val_pred = model.predict(X_val)\n",
    "test_pred = model.predict(X_test)\n",
    "\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "\n",
    "print(\"\\n========== RMSE Summary ==========\")\n",
    "print(f\"Train RMSE: {train_rmse:.4f}\")\n",
    "print(f\"Val RMSE:   {val_rmse:.4f}\")\n",
    "print(f\"Test RMSE:  {test_rmse:.4f}\")\n",
    "print(\"==================================\\n\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 8️⃣ Training vs Validation Loss Visualization\n",
    "# ------------------------------------------------------------\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')\n",
    "plt.title(\"Training vs Validation Loss (AlphaWave vX.3)\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 9️⃣ Evaluate Per Stock + Visualize Predictions\n",
    "# ------------------------------------------------------------\n",
    "results = []\n",
    "\n",
    "for t in tickers:\n",
    "    X_stock, y_stock = create_sequences(scaled_data[t], seq_len)\n",
    "    pred_stock = model.predict(X_stock)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_stock, pred_stock))\n",
    "    results.append({'Stock': t, 'RMSE': rmse})\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(y_stock[-500:], label='Actual', color='blue')\n",
    "    plt.plot(pred_stock[-500:], label='Predicted', color='red')\n",
    "    plt.title(f\"{t} Stock Prediction (AlphaWave vX.3)\")\n",
    "    plt.xlabel(\"Days\")\n",
    "    plt.ylabel(\"Standardized Price\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nPer-stock RMSE summary:\\n\", results_df)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 🔟 (Optional) Save Model + Metrics\n",
    "# ------------------------------------------------------------\n",
    "# model.save(\"alphawave_vX3_optimized_lstm.h5\")\n",
    "# results_df.to_csv(\"alphawave_vX3_rmse_summary.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac7c110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Save trained AlphaWave model + metrics (post-training)\n",
    "# ------------------------------------------------------------\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "model_name = f\"alphawave_vX3_trained_{timestamp}.keras\"\n",
    "metrics_name = f\"alphawave_vX3_metrics_{timestamp}.csv\"\n",
    "\n",
    "# Save the model in Keras format (recommended)\n",
    "model.save(model_name)\n",
    "print(f\"✅ Model saved successfully as '{model_name}'\")\n",
    "\n",
    "# Optional: if RMSE variables exist, save them too\n",
    "try:\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Metric': ['Train_RMSE', 'Val_RMSE', 'Test_RMSE'],\n",
    "        'Value': [train_rmse, val_rmse, test_rmse]\n",
    "    })\n",
    "    metrics_df.to_csv(metrics_name, index=False)\n",
    "    print(f\"📊 Metrics saved as '{metrics_name}'\")\n",
    "except NameError:\n",
    "    print(\"⚠️ RMSE metrics not found — skipping metrics save.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca92cadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# Save all stock scalers for AlphaWave vX.3\n",
    "# ===========================================================\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Create a dedicated folder for scalers\n",
    "os.makedirs(\"alphawave_scalers\", exist_ok=True)\n",
    "\n",
    "# Loop through each stock and save its StandardScaler\n",
    "for t in tickers:\n",
    "    file_path = os.path.join(\"alphawave_scalers\", f\"scaler_{t}.pkl\")\n",
    "    joblib.dump(scalers[t], file_path)\n",
    "\n",
    "print(f\"✅ All {len(tickers)} stock scalers saved successfully in the 'alphawave_scalers/' folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e5220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# AlphaWave vX.3 Generalization & Future Data Evaluation\n",
    "# ===========================================================\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 1️⃣ Train vs Validation RMSE Ratio (Overfitting Metric)\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "print(\"========== Train vs Validation RMSE Ratio ==========\")\n",
    "\n",
    "try:\n",
    "    train_rmse_ratio = train_rmse / val_rmse\n",
    "    print(f\"Train RMSE:       {train_rmse:.4f}\")\n",
    "    print(f\"Validation RMSE:  {val_rmse:.4f}\")\n",
    "    print(f\"RMSE Ratio:       {train_rmse_ratio:.3f}\")\n",
    "\n",
    "    if train_rmse_ratio < 1.1:\n",
    "        print(\"✅ Excellent generalization — model not overfitting.\")\n",
    "    elif 1.1 <= train_rmse_ratio <= 1.3:\n",
    "        print(\"⚖️  Mild overfitting, acceptable range.\")\n",
    "    else:\n",
    "        print(\"⚠️  Possible overfitting — model fits training data too tightly.\")\n",
    "except NameError:\n",
    "    print(\"⚠️ RMSE variables not found. Skipping this part.\")\n",
    "    train_rmse_ratio = None\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2️⃣ Evaluate on Unseen 2025 Data (Future Drift Test)\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "print(\"\\n========== Future Data Evaluation (2025 onward) ==========\")\n",
    "\n",
    "future_results = []\n",
    "\n",
    "for t in tickers:\n",
    "    print(f\"\\nEvaluating {t}...\")\n",
    "    try:\n",
    "        # Download unseen data\n",
    "        future_data = yf.download(t, start='2025-01-01', end=datetime.today().strftime('%Y-%m-%d'))\n",
    "        if len(future_data) < 60:\n",
    "            print(f\"⚠️  Not enough 2025 data for {t}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Preprocess using saved scaler\n",
    "        scaled_future = scalers[t].transform(future_data[['Close', 'Volume']])\n",
    "        \n",
    "        # Create sequences\n",
    "        X_future, y_future = [], []\n",
    "        for i in range(seq_len, len(scaled_future)):\n",
    "            X_future.append(scaled_future[i - seq_len:i])\n",
    "            y_future.append(scaled_future[i, 0])\n",
    "        X_future, y_future = np.array(X_future), np.array(y_future)\n",
    "\n",
    "        # Predict & evaluate\n",
    "        future_pred = model.predict(X_future, verbose=0)\n",
    "        future_rmse = np.sqrt(mean_squared_error(y_future, future_pred))\n",
    "        future_results.append({'Stock': t, 'Future_RMSE': future_rmse})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error evaluating {t}: {e}\")\n",
    "\n",
    "# Create summary DataFrame\n",
    "future_df = pd.DataFrame(future_results)\n",
    "print(\"\\n========== Future RMSE Summary ==========\")\n",
    "print(future_df)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 3️⃣ Compare test vs future RMSE (Model Drift)\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "if not future_df.empty:\n",
    "    try:\n",
    "        avg_future_rmse = future_df['Future_RMSE'].mean()\n",
    "        print(\"\\n========== Model Drift Summary ==========\")\n",
    "        print(f\"Average Test RMSE (from training):  {test_rmse:.4f}\")\n",
    "        print(f\"Average Future RMSE (2025 unseen):  {avg_future_rmse:.4f}\")\n",
    "        drift_ratio = avg_future_rmse / test_rmse\n",
    "        print(f\"RMSE Drift Ratio: {drift_ratio:.3f}\")\n",
    "\n",
    "        if drift_ratio < 1.2:\n",
    "            print(\"✅ Model generalizes stably across unseen 2025 data.\")\n",
    "        elif 1.2 <= drift_ratio <= 1.5:\n",
    "            print(\"⚖️  Moderate drift — retraining in 6 months recommended.\")\n",
    "        else:\n",
    "            print(\"⚠️  Significant drift — consider retraining with recent data.\")\n",
    "    except NameError:\n",
    "        print(\"⚠️ Test RMSE not found. Only showing future RMSEs.\")\n",
    "else:\n",
    "    print(\"⚠️ No valid results for future data evaluation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c15dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# AlphaWave vX.4 — Retraining with 2025 Data (Full Pipeline)\n",
    "# ===========================================================\n",
    "\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 1️⃣ Setup — Stock List and Configs\n",
    "# -----------------------------------------------------------\n",
    "tickers = [\n",
    "    'AAPL', 'MSFT', 'AMZN', 'GOOG', 'NVDA', 'META', 'TSLA', 'AMD', 'ADBE', 'NFLX',\n",
    "    'INTC', 'IBM', 'PYPL', 'ORCL', 'CSCO', 'CRM', 'QCOM', 'AVGO', 'TXN', 'SHOP',\n",
    "    'UBER', 'SNOW', 'BABA', 'PLTR', 'COIN'\n",
    "]\n",
    "\n",
    "seq_len = 60  # Lookback window\n",
    "start_date = '2015-01-01'\n",
    "end_date = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "os.makedirs(\"alphawave_vX4_checkpoints\", exist_ok=True)\n",
    "os.makedirs(\"alphawave_vX4_scalers\", exist_ok=True)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2️⃣ Data Loading and Preprocessing\n",
    "# -----------------------------------------------------------\n",
    "def preprocess_data(df):\n",
    "    df = df[['Close', 'Volume']].dropna()\n",
    "    scaler = StandardScaler()\n",
    "    scaled = scaler.fit_transform(df)\n",
    "    return scaled, scaler\n",
    "\n",
    "data_dict, scaled_data, scalers = {}, {}, {}\n",
    "\n",
    "print(\"📈 Fetching and preprocessing updated stock data...\\n\")\n",
    "for t in tickers:\n",
    "    df = yf.download(t, start=start_date, end=end_date)\n",
    "    if len(df) > 0:\n",
    "        data_dict[t] = df\n",
    "        scaled_data[t], scalers[t] = preprocess_data(df)\n",
    "    else:\n",
    "        print(f\"⚠️ Skipping {t}: no data available.\")\n",
    "\n",
    "# Save updated scalers\n",
    "for t in tickers:\n",
    "    joblib.dump(scalers[t], f\"alphawave_vX4_scalers/scaler_{t}.pkl\")\n",
    "\n",
    "print(\"\\n✅ All scalers updated and saved successfully.\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 3️⃣ Create Training Dataset\n",
    "# -----------------------------------------------------------\n",
    "def create_dataset(data_dict, seq_len=60):\n",
    "    X, y = [], []\n",
    "    for t in data_dict.keys():\n",
    "        data = scaled_data[t]\n",
    "        for i in range(seq_len, len(data)):\n",
    "            X.append(data[i - seq_len:i])\n",
    "            y.append(data[i, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = create_dataset(scaled_data, seq_len=seq_len)\n",
    "split1, split2 = int(0.7 * len(X)), int(0.85 * len(X))\n",
    "\n",
    "X_train, y_train = X[:split1], y[:split1]\n",
    "X_val, y_val = X[split1:split2], y[split1:split2]\n",
    "X_test, y_test = X[split2:], y[split2:]\n",
    "\n",
    "print(f\"📊 Dataset created:\")\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4️⃣ Build the Updated LSTM Model\n",
    "# -----------------------------------------------------------\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(seq_len, X.shape[2])),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    LSTM(32, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "optimizer = Adam(learning_rate=0.0005)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "model.summary()\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 5️⃣ Callbacks for Stable Training\n",
    "# -----------------------------------------------------------\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-5),\n",
    "    ModelCheckpoint(\n",
    "        filepath=\"alphawave_vX4_checkpoints/best_model.keras\",\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 6️⃣ Train the Model\n",
    "# -----------------------------------------------------------\n",
    "print(\"\\n🚀 Training AlphaWave vX.4 with updated data...\\n\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 7️⃣ Evaluate and Compare\n",
    "# -----------------------------------------------------------\n",
    "train_pred = model.predict(X_train)\n",
    "val_pred = model.predict(X_val)\n",
    "test_pred = model.predict(X_test)\n",
    "\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "\n",
    "print(\"\\n========== vX.4 RMSE Summary ==========\")\n",
    "print(f\"Train RMSE: {train_rmse:.4f}\")\n",
    "print(f\"Validation RMSE: {val_rmse:.4f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 8️⃣ Save Final Model and Metrics\n",
    "# -----------------------------------------------------------\n",
    "model.save(\"alphawave_vX4_checkpoints/AlphaWave_vX4.keras\")\n",
    "\n",
    "metrics = pd.DataFrame({\n",
    "    'Metric': ['Train_RMSE', 'Val_RMSE', 'Test_RMSE'],\n",
    "    'Value': [train_rmse, val_rmse, test_rmse]\n",
    "})\n",
    "metrics.to_csv(\"alphawave_vX4_checkpoints/metrics_vX4.csv\", index=False)\n",
    "\n",
    "print(\"\\n💾 Model and metrics saved to 'alphawave_vX4_checkpoints/'\")\n",
    "print(\"🎯 AlphaWave vX.4 retraining completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ea48ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Predict and plot for AAPL\n",
    "t = \"AAPL\"\n",
    "df = data_dict[t]\n",
    "scaled = scalers[t].transform(df[['Close', 'Volume']])\n",
    "X_stock = np.array([scaled[i - seq_len:i] for i in range(seq_len, len(scaled))])\n",
    "preds = model.predict(X_stock, verbose=0)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(df.index[seq_len:], scaled[seq_len:, 0], label='Actual (Scaled)', color='blue')\n",
    "plt.plot(df.index[seq_len:], preds, label='Predicted', color='red')\n",
    "plt.title(f\"{t} — AlphaWave vX.4 Stock Prediction\")\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Standardized Close Price')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3b50e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Normalize index lengths for plotting\n",
    "train_range = range(len(train_pred))\n",
    "val_range = range(len(train_pred), len(train_pred) + len(val_pred))\n",
    "test_range = range(len(train_pred) + len(val_pred),\n",
    "                    len(train_pred) + len(val_pred) + len(test_pred))\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(train_range, y_train, label='Train Actual', color='blue', alpha=0.4)\n",
    "plt.plot(train_range, train_pred, label='Train Predicted', color='cyan', alpha=0.6)\n",
    "plt.plot(val_range, y_val, label='Val Actual', color='green', alpha=0.4)\n",
    "plt.plot(val_range, val_pred, label='Val Predicted', color='lime', alpha=0.6)\n",
    "plt.plot(test_range, y_test, label='Test Actual', color='red', alpha=0.4)\n",
    "plt.plot(test_range, test_pred, label='Test Predicted', color='orange', alpha=0.6)\n",
    "\n",
    "plt.title(\"AlphaWave vX.4 — Training vs Validation vs Test Prediction\")\n",
    "plt.xlabel(\"Sequence Index\")\n",
    "plt.ylabel(\"Standardized Price\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d74571",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train RMSE: {train_rmse:.4f}\")\n",
    "print(f\"Validation RMSE: {val_rmse:.4f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702de7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# AlphaWave vX.3 Generalization & Future Data Evaluation\n",
    "# ===========================================================\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 1️⃣ Train vs Validation RMSE Ratio (Overfitting Metric)\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "print(\"========== Train vs Validation RMSE Ratio ==========\")\n",
    "\n",
    "try:\n",
    "    train_rmse_ratio = train_rmse / val_rmse\n",
    "    print(f\"Train RMSE:       {train_rmse:.4f}\")\n",
    "    print(f\"Validation RMSE:  {val_rmse:.4f}\")\n",
    "    print(f\"RMSE Ratio:       {train_rmse_ratio:.3f}\")\n",
    "\n",
    "    if train_rmse_ratio < 1.1:\n",
    "        print(\"✅ Excellent generalization — model not overfitting.\")\n",
    "    elif 1.1 <= train_rmse_ratio <= 1.3:\n",
    "        print(\"⚖️  Mild overfitting, acceptable range.\")\n",
    "    else:\n",
    "        print(\"⚠️  Possible overfitting — model fits training data too tightly.\")\n",
    "except NameError:\n",
    "    print(\"⚠️ RMSE variables not found. Skipping this part.\")\n",
    "    train_rmse_ratio = None\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2️⃣ Evaluate on Unseen 2025 Data (Future Drift Test)\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "print(\"\\n========== Future Data Evaluation (2025 onward) ==========\")\n",
    "\n",
    "future_results = []\n",
    "\n",
    "for t in tickers:\n",
    "    print(f\"\\nEvaluating {t}...\")\n",
    "    try:\n",
    "        # Download unseen data\n",
    "        future_data = yf.download(t, start='2025-01-01', end=datetime.today().strftime('%Y-%m-%d'))\n",
    "        if len(future_data) < 60:\n",
    "            print(f\"⚠️  Not enough 2025 data for {t}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Preprocess using saved scaler\n",
    "        scaled_future = scalers[t].transform(future_data[['Close', 'Volume']])\n",
    "        \n",
    "        # Create sequences\n",
    "        X_future, y_future = [], []\n",
    "        for i in range(seq_len, len(scaled_future)):\n",
    "            X_future.append(scaled_future[i - seq_len:i])\n",
    "            y_future.append(scaled_future[i, 0])\n",
    "        X_future, y_future = np.array(X_future), np.array(y_future)\n",
    "\n",
    "        # Predict & evaluate\n",
    "        future_pred = model.predict(X_future, verbose=0)\n",
    "        future_rmse = np.sqrt(mean_squared_error(y_future, future_pred))\n",
    "        future_results.append({'Stock': t, 'Future_RMSE': future_rmse})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error evaluating {t}: {e}\")\n",
    "\n",
    "# Create summary DataFrame\n",
    "future_df = pd.DataFrame(future_results)\n",
    "print(\"\\n========== Future RMSE Summary ==========\")\n",
    "print(future_df)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 3️⃣ Compare test vs future RMSE (Model Drift)\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "if not future_df.empty:\n",
    "    try:\n",
    "        avg_future_rmse = future_df['Future_RMSE'].mean()\n",
    "        print(\"\\n========== Model Drift Summary ==========\")\n",
    "        print(f\"Average Test RMSE (from training):  {test_rmse:.4f}\")\n",
    "        print(f\"Average Future RMSE (2025 unseen):  {avg_future_rmse:.4f}\")\n",
    "        drift_ratio = avg_future_rmse / test_rmse\n",
    "        print(f\"RMSE Drift Ratio: {drift_ratio:.3f}\")\n",
    "\n",
    "        if drift_ratio < 1.2:\n",
    "            print(\"✅ Model generalizes stably across unseen 2025 data.\")\n",
    "        elif 1.2 <= drift_ratio <= 1.5:\n",
    "            print(\"⚖️  Moderate drift — retraining in 6 months recommended.\")\n",
    "        else:\n",
    "            print(\"⚠️  Significant drift — consider retraining with recent data.\")\n",
    "    except NameError:\n",
    "        print(\"⚠️ Test RMSE not found. Only showing future RMSEs.\")\n",
    "else:\n",
    "    print(\"⚠️ No valid results for future data evaluation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa11454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AlphaWave vX.5 – Optimized Universal LSTM (Overfitting Controlled)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# ===============================\n",
    "# 1. Load Multiple Stock Data\n",
    "# ===============================\n",
    "tickers = [\n",
    "    'AAPL', 'MSFT', 'AMZN', 'GOOG', 'NVDA', 'META', 'TSLA', 'AMD',\n",
    "    'ADBE', 'NFLX', 'INTC', 'IBM', 'PYPL', 'ORCL', 'CSCO', 'CRM',\n",
    "    'QCOM', 'AVGO', 'TXN', 'SHOP', 'UBER', 'SNOW', 'BABA', 'PLTR', 'COIN'\n",
    "]\n",
    "\n",
    "data = yf.download(tickers, start='2018-01-01', end='2025-01-01')['Close']\n",
    "data = data.fillna(method='ffill').dropna()\n",
    "\n",
    "# ===============================\n",
    "# 2. Data Preprocessing\n",
    "# ===============================\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "lookback = 60\n",
    "X, y = [], []\n",
    "\n",
    "for i in range(lookback, len(scaled_data)):\n",
    "    X.append(scaled_data[i-lookback:i])\n",
    "    y.append(scaled_data[i])\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "# Split data (temporal)\n",
    "train_size = int(len(X) * 0.7)\n",
    "val_size = int(len(X) * 0.15)\n",
    "\n",
    "X_train, X_val, X_test = (\n",
    "    X[:train_size],\n",
    "    X[train_size:train_size + val_size],\n",
    "    X[train_size + val_size:]\n",
    ")\n",
    "y_train, y_val, y_test = (\n",
    "    y[:train_size],\n",
    "    y[train_size:train_size + val_size],\n",
    "    y[train_size + val_size:]\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# 3. Build Optimized LSTM Model\n",
    "# ===============================\n",
    "model = Sequential([\n",
    "    LSTM(48, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "         dropout=0.3, recurrent_dropout=0.2, kernel_regularizer=l2(0.001)),\n",
    "    LSTM(32, return_sequences=False, dropout=0.3, recurrent_dropout=0.2,\n",
    "         kernel_regularizer=l2(0.001)),\n",
    "    Dense(32, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.3),\n",
    "    Dense(y_train.shape[1])  # Predict all stocks simultaneously\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()\n",
    "\n",
    "# ===============================\n",
    "# 4. Train with EarlyStopping\n",
    "# ===============================\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# 5. Evaluate Performance\n",
    "# ===============================\n",
    "train_pred = model.predict(X_train)\n",
    "val_pred = model.predict(X_val)\n",
    "test_pred = model.predict(X_test)\n",
    "\n",
    "train_rmse = math.sqrt(mean_squared_error(y_train, train_pred))\n",
    "val_rmse = math.sqrt(mean_squared_error(y_val, val_pred))\n",
    "test_rmse = math.sqrt(mean_squared_error(y_test, test_pred))\n",
    "\n",
    "print(f\"Train RMSE: {train_rmse:.4f}\")\n",
    "print(f\"Validation RMSE: {val_rmse:.4f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "# ===============================\n",
    "# 6. Plot Train vs Validation vs Test Predictions\n",
    "# ===============================\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_train.flatten()[:1000], label='Train Actual', color='blue', alpha=0.4)\n",
    "plt.plot(train_pred.flatten()[:1000], label='Train Predicted', color='cyan', alpha=0.4)\n",
    "plt.plot(y_val.flatten()[:500], label='Val Actual', color='green', alpha=0.4)\n",
    "plt.plot(val_pred.flatten()[:500], label='Val Predicted', color='lime', alpha=0.4)\n",
    "plt.plot(y_test.flatten()[:500], label='Test Actual', color='orange', alpha=0.5)\n",
    "plt.plot(test_pred.flatten()[:500], label='Test Predicted', color='red', alpha=0.5)\n",
    "plt.title(\"AlphaWave vX.5 — Training vs Validation vs Test Prediction\")\n",
    "plt.xlabel(\"Sequence Index\")\n",
    "plt.ylabel(\"Standardized Price\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ===============================\n",
    "# 7. Save Model and Scaler\n",
    "# ===============================\n",
    "model.save(\"alphawave_vX5_optimized.keras\")\n",
    "import pickle\n",
    "with open(\"scaler_vX5.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"✅ Model and scaler saved as 'alphawave_vX5_optimized.keras' and 'scaler_vX5.pkl'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00122be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# AlphaWave vX.6 — Stock-Aware GRU Hybrid\n",
    "# ===========================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, GRU, Dense, Dropout, Concatenate, Flatten\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "# ===========================================\n",
    "# 1. Load and Prepare Multi-Stock Data\n",
    "# ===========================================\n",
    "tickers = [\n",
    "    'AAPL', 'MSFT', 'AMZN', 'GOOG', 'NVDA', 'META', 'TSLA', 'AMD',\n",
    "    'ADBE', 'NFLX', 'INTC', 'IBM', 'PYPL', 'ORCL', 'CSCO', 'CRM',\n",
    "    'QCOM', 'AVGO', 'TXN', 'SHOP', 'UBER', 'SNOW', 'BABA', 'PLTR', 'COIN'\n",
    "]\n",
    "\n",
    "data = yf.download(tickers, start='2018-01-01', end='2025-01-01')['Close']\n",
    "data = data.fillna(method='ffill').dropna()\n",
    "\n",
    "# ===========================================\n",
    "# 2. Per-Stock Normalization (Independent Scaling)\n",
    "# ===========================================\n",
    "scalers = {}\n",
    "scaled_data = pd.DataFrame(index=data.index)\n",
    "\n",
    "for col in data.columns:\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data[col] = scaler.fit_transform(data[[col]])\n",
    "    scalers[col] = scaler\n",
    "\n",
    "lookback = 40\n",
    "X, y, stock_ids = [], [], []\n",
    "\n",
    "# Convert to sequences\n",
    "for stock_idx, col in enumerate(data.columns):\n",
    "    stock_series = scaled_data[col].values\n",
    "    for i in range(lookback, len(stock_series)):\n",
    "        X.append(stock_series[i-lookback:i])\n",
    "        y.append(stock_series[i])\n",
    "        stock_ids.append(stock_idx)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "stock_ids = np.array(stock_ids)\n",
    "\n",
    "# Reshape for GRU: (samples, timesteps, features)\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "\n",
    "# Train/Val/Test split\n",
    "train_size = int(len(X) * 0.7)\n",
    "val_size = int(len(X) * 0.15)\n",
    "\n",
    "X_train, X_val, X_test = X[:train_size], X[train_size:train_size+val_size], X[train_size+val_size:]\n",
    "y_train, y_val, y_test = y[:train_size], y[train_size:train_size+val_size], y[train_size+val_size:]\n",
    "stock_train, stock_val, stock_test = stock_ids[:train_size], stock_ids[train_size:train_size+val_size], stock_ids[train_size+val_size:]\n",
    "\n",
    "# ===========================================\n",
    "# 3. Build Stock-Aware GRU Model\n",
    "# ===========================================\n",
    "num_stocks = len(tickers)\n",
    "embedding_dim = 8  # Compact but powerful stock representation\n",
    "\n",
    "# Inputs\n",
    "price_input = Input(shape=(lookback, 1), name=\"price_sequence\")\n",
    "stock_input = Input(shape=(1,), name=\"stock_id\")\n",
    "\n",
    "# Embedding Layer\n",
    "embedding = Embedding(input_dim=num_stocks, output_dim=embedding_dim, name=\"stock_embedding\")(stock_input)\n",
    "embedding_flat = Flatten()(embedding)\n",
    "\n",
    "# GRU Stack\n",
    "\n",
    "gru_out = GRU(48, return_sequences=True, dropout=0.2, kernel_regularizer=l2(0.001))(price_input)\n",
    "gru_out = GRU(32, return_sequences=False, dropout=0.2, kernel_regularizer=l2(0.001))(gru_out)\n",
    "\n",
    "\n",
    "# Concatenate stock embedding + GRU output\n",
    "merged = Concatenate()([gru_out, embedding_flat])\n",
    "\n",
    "# Dense Layers\n",
    "dense_out = Dense(64, activation='relu', kernel_regularizer=l2(0.001))(merged)\n",
    "dense_out = Dropout(0.3)(dense_out)\n",
    "output = Dense(1, activation='linear')(dense_out)\n",
    "\n",
    "# Compile Model\n",
    "model = Model(inputs=[price_input, stock_input], outputs=output)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# ===========================================\n",
    "# 4. Train with EarlyStopping + LR Scheduler\n",
    "# ===========================================\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1)\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train, stock_train], y_train,\n",
    "    validation_data=([X_val, stock_val], y_val),\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop, lr_scheduler],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ===========================================\n",
    "# 5. Evaluate and Plot\n",
    "# ===========================================\n",
    "train_pred = model.predict([X_train, stock_train])\n",
    "val_pred = model.predict([X_val, stock_val])\n",
    "test_pred = model.predict([X_test, stock_test])\n",
    "\n",
    "train_rmse = math.sqrt(mean_squared_error(y_train, train_pred))\n",
    "val_rmse = math.sqrt(mean_squared_error(y_val, val_pred))\n",
    "test_rmse = math.sqrt(mean_squared_error(y_test, test_pred))\n",
    "\n",
    "print(f\"Train RMSE: {train_rmse:.4f}\")\n",
    "print(f\"Validation RMSE: {val_rmse:.4f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(y_test[:800], label='Actual', color='blue', alpha=0.6)\n",
    "plt.plot(test_pred[:800], label='Predicted', color='red', alpha=0.6)\n",
    "plt.title(\"AlphaWave vX.6 — Stock-Aware GRU Predictions\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Standardized Price\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ===========================================\n",
    "# 6. Save Model and Scalers\n",
    "# ===========================================\n",
    "model.save(\"alphawave_vX6_stockaware.keras\")\n",
    "\n",
    "with open(\"scalers_vX6.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scalers, f)\n",
    "\n",
    "print(\"✅ Model and scalers saved successfully as 'alphawave_vX6_stockaware.keras' and 'scalers_vX6.pkl'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5c5a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_stock_predictions_and_forecast(stock_symbol, model, data, scalers, lookback=40, forecast_horizon=30):\n",
    "    \"\"\"\n",
    "    Visualizes past predictions vs actual data and 30-day forecast for an existing trained multi-input GRU model.\n",
    "    Compatible with models taking [price_sequence, stock_id].\n",
    "    \"\"\"\n",
    "    # --- Step 1: Prepare data for the selected stock ---\n",
    "    prices = data[stock_symbol].values.reshape(-1, 1)\n",
    "    scaler = scalers[stock_symbol]\n",
    "    scaled_prices = scaler.transform(prices)\n",
    "\n",
    "    # Create sequences (X_test, y_test)\n",
    "    X_test, y_test = [], []\n",
    "    for i in range(lookback, len(scaled_prices)):\n",
    "        X_test.append(scaled_prices[i - lookback:i])\n",
    "        y_test.append(scaled_prices[i])\n",
    "    X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "\n",
    "    # Encode stock ID properly\n",
    "    stock_index = list(data.columns).index(stock_symbol)\n",
    "    stock_input = np.full((len(X_test), 1), stock_index, dtype=np.int32)\n",
    "\n",
    "    # --- Step 2: Predict full historical sequence ---\n",
    "    # Ensure both inputs are proper numpy arrays with same batch dimension\n",
    "    predicted_scaled = model.predict([X_test, stock_input], verbose=0)\n",
    "\n",
    "    # --- Step 3: Inverse transform for readability ---\n",
    "    actual_prices = scaler.inverse_transform(y_test)\n",
    "    predicted_prices = scaler.inverse_transform(predicted_scaled)\n",
    "\n",
    "    # --- Step 4: Plot Actual vs Predicted (Historical) ---\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(actual_prices, color='blue', label='Actual')\n",
    "    plt.plot(predicted_prices, color='red', label='Predicted')\n",
    "    plt.title(f\"{stock_symbol} Stock Prediction (AlphaWave vX.6)\")\n",
    "    plt.xlabel(\"Days\")\n",
    "    plt.ylabel(\"Price (USD)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # --- Step 5: Forecast Next 30 Days ---\n",
    "    last_seq = scaled_prices[-lookback:].reshape(1, lookback, 1)\n",
    "    stock_input_future = np.array([[stock_index]], dtype=np.int32)\n",
    "    future_preds_scaled = []\n",
    "\n",
    "    for _ in range(forecast_horizon):\n",
    "        next_pred_scaled = model.predict([last_seq, stock_input_future], verbose=0)[0, 0]\n",
    "        future_preds_scaled.append(next_pred_scaled)\n",
    "        # Update rolling window\n",
    "        last_seq = np.append(last_seq[:, 1:, :], [[[next_pred_scaled]]], axis=1)\n",
    "\n",
    "    future_preds_scaled = np.array(future_preds_scaled).reshape(-1, 1)\n",
    "    future_prices = scaler.inverse_transform(future_preds_scaled)\n",
    "\n",
    "    # --- Step 6: Plot Forecast ---\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(range(1, forecast_horizon + 1), future_prices, marker='o', color='green')\n",
    "    plt.title(f\"{stock_symbol} - 30-Day Forecast (AlphaWave vX.6)\")\n",
    "    plt.xlabel(\"Days Ahead\")\n",
    "    plt.ylabel(\"Predicted Price (USD)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Final predicted price after {forecast_horizon} days: ${future_prices[-1][0]:.2f}\")\n",
    "\n",
    "# ✅ Example usage\n",
    "for ticker in [\"AAPL\", \"TSLA\", \"GOOG\", \"AMZN\"]:\n",
    "    plot_stock_predictions_and_forecast(ticker, model, data, scalers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd35922e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alphawave",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
